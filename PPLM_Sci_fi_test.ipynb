{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLda6-tP8nAY",
        "outputId": "287ec613-b178-4e61-8e01-82f4b0b3fd38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing requirements.txt\n"
          ]
        }
      ],
      "source": [
        "%%writefile requirements.txt\n",
        "torch==1.7.0\n",
        "nltk==3.4.5\n",
        "colorama==0.4.4\n",
        "transformers==3.4.0\n",
        "torchtext==0.3.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38Jzw7eak0m1",
        "outputId": "64b1d317-21d6-4b1a-d910-3df7586c787d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==1.7.0\n",
            "  Downloading torch-1.7.0-cp37-cp37m-manylinux1_x86_64.whl (776.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 776.7 MB 4.6 kB/s \n",
            "\u001b[?25hCollecting nltk==3.4.5\n",
            "  Downloading nltk-3.4.5.zip (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 70.9 MB/s \n",
            "\u001b[?25hCollecting colorama==0.4.4\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Collecting transformers==3.4.0\n",
            "  Downloading transformers-3.4.0-py3-none-any.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 56.1 MB/s \n",
            "\u001b[?25hCollecting torchtext==0.3.1\n",
            "  Downloading torchtext-0.3.1-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.1 MB/s \n",
            "\u001b[?25hCollecting dataclasses\n",
            "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0->-r requirements.txt (line 1)) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0->-r requirements.txt (line 1)) (4.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0->-r requirements.txt (line 1)) (1.21.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.4.5->-r requirements.txt (line 2)) (1.15.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->-r requirements.txt (line 4)) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->-r requirements.txt (line 4)) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->-r requirements.txt (line 4)) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->-r requirements.txt (line 4)) (21.3)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 50.1 MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.9.2\n",
            "  Downloading tokenizers-0.9.2-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 59.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->-r requirements.txt (line 4)) (3.17.3)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 52.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->-r requirements.txt (line 4)) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.4.0->-r requirements.txt (line 4)) (3.0.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.4.0->-r requirements.txt (line 4)) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.4.0->-r requirements.txt (line 4)) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.4.0->-r requirements.txt (line 4)) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.4.0->-r requirements.txt (line 4)) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.4.0->-r requirements.txt (line 4)) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.4.0->-r requirements.txt (line 4)) (1.1.0)\n",
            "Building wheels for collected packages: nltk, sacremoses\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.4.5-py3-none-any.whl size=1449924 sha256=62d68692578af1b8befb2e97b7c22de0b7cd485452ecbadfc9169ddff4545338\n",
            "  Stored in directory: /root/.cache/pip/wheels/48/8b/7f/473521e0c731c6566d631b281f323842bbda9bd819eb9a3ead\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=46cc4b6c1c8c118ea2e387c6bdff561c5835325a885678cb0504b9916237987d\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built nltk sacremoses\n",
            "Installing collected packages: dataclasses, torch, tokenizers, sentencepiece, sacremoses, transformers, torchtext, nltk, colorama\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.11.0+cu113\n",
            "    Uninstalling torch-1.11.0+cu113:\n",
            "      Successfully uninstalled torch-1.11.0+cu113\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.12.0\n",
            "    Uninstalling torchtext-0.12.0:\n",
            "      Successfully uninstalled torchtext-0.12.0\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.12.0+cu113 requires torch==1.11.0, but you have torch 1.7.0 which is incompatible.\n",
            "torchaudio 0.11.0+cu113 requires torch==1.11.0, but you have torch 1.7.0 which is incompatible.\u001b[0m\n",
            "Successfully installed colorama-0.4.4 dataclasses-0.6 nltk-3.4.5 sacremoses-0.0.53 sentencepiece-0.1.96 tokenizers-0.9.2 torch-1.7.0 torchtext-0.3.1 transformers-3.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyVMT11KiEOT",
        "outputId": "12b1a6fa-adb8-4b7a-cc07-c1c59d4775c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing pplm_classification_head.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile pplm_classification_head.py\n",
        "#! /usr/bin/env python3\n",
        "from torch import nn\n",
        "\n",
        "class ClassificationHead(nn.Module):\n",
        "    \"\"\"Classification Head for  transformer encoders\"\"\"\n",
        "\n",
        "    def __init__(self, class_size, embed_size):\n",
        "        super().__init__()\n",
        "        self.class_size = class_size\n",
        "        self.embed_size = embed_size\n",
        "        # self.mlp1 = nn.Linear(embed_size, embed_size)\n",
        "        # self.mlp2 = (nn.Linear(embed_size, class_size))\n",
        "        self.mlp = nn.Linear(embed_size, class_size)\n",
        "\n",
        "    def forward(self, hidden_state):\n",
        "        # hidden_state = nn.functional.relu(self.mlp1(hidden_state))\n",
        "        # hidden_state = self.mlp2(hidden_state)\n",
        "        logits = self.mlp(hidden_state)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noSymyAUfVwa",
        "outputId": "152546a7-4974-4f43-8fda-72150cc8fcff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing run_pplm.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile run_pplm.py\n",
        "#! /usr/bin/env python3\n",
        "# coding=utf-8\n",
        "# Copyright 2018 The Uber AI Team Authors.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "\"\"\"\n",
        "Example command with bag of words:\n",
        "python examples/run_pplm.py -B space --cond_text \"The president\" --length 100 --gamma 1.5 --num_iterations 3 --num_samples 10 --stepsize 0.01 --window_length 5 --kl_scale 0.01 --gm_scale 0.95\n",
        "\n",
        "Example command with discriminator:\n",
        "python examples/run_pplm.py -D sentiment --class_label 3 --cond_text \"The lake\" --length 10 --gamma 1.0 --num_iterations 30 --num_samples 10 --stepsize 0.01 --kl_scale 0.01 --gm_scale 0.95\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "import json\n",
        "from operator import add\n",
        "from typing import List, Optional, Tuple, Union\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from tqdm import trange\n",
        "from transformers import GPT2Tokenizer\n",
        "from transformers.file_utils import cached_path\n",
        "from transformers.modeling_gpt2 import GPT2LMHeadModel\n",
        "\n",
        "from pplm_classification_head import ClassificationHead\n",
        "\n",
        "PPLM_BOW = 1\n",
        "PPLM_DISCRIM = 2\n",
        "PPLM_BOW_DISCRIM = 3\n",
        "SMALL_CONST = 1e-15\n",
        "BIG_CONST = 1e10\n",
        "\n",
        "QUIET = 0\n",
        "REGULAR = 1\n",
        "VERBOSE = 2\n",
        "VERY_VERBOSE = 3\n",
        "VERBOSITY_LEVELS = {\n",
        "    'quiet': QUIET,\n",
        "    'regular': REGULAR,\n",
        "    'verbose': VERBOSE,\n",
        "    'very_verbose': VERY_VERBOSE,\n",
        "}\n",
        "\n",
        "BAG_OF_WORDS_ARCHIVE_MAP = {\n",
        "    'politics': \"https://pplm.s3.amazonaws.com/pplm_wordset_cleaned.txt\",\n",
        "    'scifi': \"https://pplm.s3.amazonaws.com/pplm_wordset_cleaned.txt\"\n",
        "}\n",
        "\n",
        "DISCRIMINATOR_MODELS_PARAMS = {\n",
        "    \"clickbait\": {\n",
        "        \"url\": \"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/discriminators/clickbait_classifier_head.pt\",\n",
        "        \"class_size\": 2,\n",
        "        \"embed_size\": 1024,\n",
        "        \"class_vocab\": {\"non_clickbait\": 0, \"clickbait\": 1},\n",
        "        \"default_class\": 1,\n",
        "        \"pretrained_model\": \"gpt2-medium\",\n",
        "    },\n",
        "    \"sentiment\": {\n",
        "        \"url\": \"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/discriminators/SST_classifier_head.pt\",\n",
        "        \"class_size\": 5,\n",
        "        \"embed_size\": 1024,\n",
        "        \"class_vocab\": {\"very_positive\": 2, \"very_negative\": 3},\n",
        "        \"default_class\": 3,\n",
        "        \"pretrained_model\": \"gpt2-medium\",\n",
        "    },\n",
        "}\n",
        "\n",
        "def beamblock(input_ids, scores):\n",
        "    for batch_idx in range(input_ids.shape[0]):\n",
        "        tokens = input_ids[batch_idx].tolist() # input tokens in modified sequence after blocking.\n",
        "        tokens_ll = [[tokens[i],tokens[i+1]] for i in range(len(tokens)-1)]\n",
        "\n",
        "        # creating the bigram using the last two words.\n",
        "        tok_1 = tokens[-2]\n",
        "        tok_2 = tokens[-1] \n",
        "        bigram = '_'.join([str(tok_1),str(tok_2)])\n",
        "\n",
        "        # creating a list of bigrams in the input sequence.\n",
        "        tokens_str = []\n",
        "        for item in tokens_ll:\n",
        "          string_ints = [str(int) for int in item]\n",
        "          tokens_str.append('_'.join(string_ints))\n",
        "        \n",
        "        # find all indices where the bigrams repeat, so that its next token's probs can be made 0.\n",
        "        hits = (idx for idx, val in enumerate(tokens_str[:-1]) if val == bigram)\n",
        "        \n",
        "        # making probability of the next token's after matching bigrams to 0.\n",
        "        for hit in hits:\n",
        "          yt = int(tokens_str[hit + 1].split('_')[-1])\n",
        "          # print('making zero')\n",
        "          # scores[batch_idx, yt] = -1e10            \n",
        "          scores[batch_idx,yt] = 0.0000000000001\n",
        "    return scores    \n",
        "\n",
        "def to_var(x, requires_grad=False, volatile=False, device='cuda'):\n",
        "    if torch.cuda.is_available() and device == 'cuda':\n",
        "        x = x.cuda()\n",
        "    elif device != 'cuda':\n",
        "        x = x.to(device)\n",
        "    return Variable(x, requires_grad=requires_grad, volatile=volatile)\n",
        "\n",
        "\n",
        "def top_k_filter(logits, k, probs=False):\n",
        "    \"\"\"\n",
        "    Masks everything but the k top entries as -infinity (1e10).\n",
        "    Used to mask logits such that e^-infinity -> 0 won't contribute to the\n",
        "    sum of the denominator.\n",
        "    \"\"\"\n",
        "    if k == 0:\n",
        "        return logits\n",
        "    else:\n",
        "        values = torch.topk(logits, k)[0]\n",
        "        batch_mins = values[:, -1].view(-1, 1).expand_as(logits)\n",
        "        if probs:\n",
        "            return torch.where(logits < batch_mins,\n",
        "                               torch.ones_like(logits) * 0.0, logits)\n",
        "        return torch.where(logits < batch_mins,\n",
        "                           torch.ones_like(logits) * -BIG_CONST,\n",
        "                           logits)\n",
        "\n",
        "\n",
        "def perturb_past(\n",
        "        past,\n",
        "        model,\n",
        "        last,\n",
        "        unpert_past=None,\n",
        "        unpert_logits=None,\n",
        "        accumulated_hidden=None,\n",
        "        grad_norms=None,\n",
        "        stepsize=0.01,\n",
        "        one_hot_bows_vectors=None,\n",
        "        classifier=None,\n",
        "        class_label=None,\n",
        "        loss_type=0,\n",
        "        num_iterations=3,\n",
        "        horizon_length=1,\n",
        "        window_length=0,\n",
        "        decay=False,\n",
        "        gamma=1.5,\n",
        "        kl_scale=0.01,\n",
        "        device='cuda',\n",
        "        verbosity_level=REGULAR\n",
        "):\n",
        "    # Generate inital perturbed past\n",
        "    grad_accumulator = [\n",
        "        (np.zeros(p.shape).astype(\"float32\"))\n",
        "        for p in past\n",
        "    ]\n",
        "\n",
        "    if accumulated_hidden is None:\n",
        "        accumulated_hidden = 0\n",
        "\n",
        "    if decay:\n",
        "        decay_mask = torch.arange(\n",
        "            0.,\n",
        "            1.0 + SMALL_CONST,\n",
        "            1.0 / (window_length)\n",
        "        )[1:]\n",
        "    else:\n",
        "        decay_mask = 1.0\n",
        "\n",
        "    # TODO fix this comment (SUMANTH)\n",
        "    # Generate a mask is gradient perturbated is based on a past window\n",
        "    _, _, _, curr_length, _ = past[0].shape\n",
        "\n",
        "    if curr_length > window_length and window_length > 0:\n",
        "        ones_key_val_shape = (\n",
        "                tuple(past[0].shape[:-2])\n",
        "                + tuple([window_length])\n",
        "                + tuple(past[0].shape[-1:])\n",
        "        )\n",
        "\n",
        "        zeros_key_val_shape = (\n",
        "                tuple(past[0].shape[:-2])\n",
        "                + tuple([curr_length - window_length])\n",
        "                + tuple(past[0].shape[-1:])\n",
        "        )\n",
        "\n",
        "        ones_mask = torch.ones(ones_key_val_shape)\n",
        "        ones_mask = decay_mask * ones_mask.permute(0, 1, 2, 4, 3)\n",
        "        ones_mask = ones_mask.permute(0, 1, 2, 4, 3)\n",
        "\n",
        "        window_mask = torch.cat(\n",
        "            (ones_mask, torch.zeros(zeros_key_val_shape)),\n",
        "            dim=-2\n",
        "        ).to(device)\n",
        "    else:\n",
        "        window_mask = torch.ones_like(past[0]).to(device)\n",
        "\n",
        "    # accumulate perturbations for num_iterations\n",
        "    loss_per_iter = []\n",
        "    new_accumulated_hidden = None\n",
        "    for i in range(num_iterations):\n",
        "        if verbosity_level >= VERBOSE:\n",
        "            print(\"Iteration \", i + 1)\n",
        "        curr_perturbation = [\n",
        "            to_var(torch.from_numpy(p_), requires_grad=True, device=device)\n",
        "            for p_ in grad_accumulator\n",
        "        ]\n",
        "\n",
        "        # Compute hidden using perturbed past\n",
        "        perturbed_past = list(map(add, past, curr_perturbation))\n",
        "        _, _, _, curr_length, _ = curr_perturbation[0].shape\n",
        "        all_logits, _, all_hidden = model(last, past=perturbed_past)\n",
        "        hidden = all_hidden[-1]\n",
        "        new_accumulated_hidden = accumulated_hidden + torch.sum(\n",
        "            hidden,\n",
        "            dim=1\n",
        "        ).detach()\n",
        "        # TODO: Check the layer-norm consistency of this with trained discriminator (Sumanth)\n",
        "        logits = all_logits[:, -1, :]\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "        loss = 0.0\n",
        "        loss_list = []\n",
        "        if loss_type == PPLM_BOW or loss_type == PPLM_BOW_DISCRIM:\n",
        "            for one_hot_bow in one_hot_bows_vectors:\n",
        "                bow_logits = torch.mm(probs, torch.t(one_hot_bow))\n",
        "                bow_loss = -torch.log(torch.sum(bow_logits))\n",
        "                loss += bow_loss\n",
        "                loss_list.append(bow_loss)\n",
        "            if verbosity_level >= VERY_VERBOSE:\n",
        "                print(\" pplm_bow_loss:\", loss.data.cpu().numpy())\n",
        "\n",
        "        if loss_type == PPLM_DISCRIM or loss_type == PPLM_BOW_DISCRIM:\n",
        "            ce_loss = torch.nn.CrossEntropyLoss()\n",
        "            # TODO why we need to do this assignment and not just using unpert_past? (Sumanth)\n",
        "            curr_unpert_past = unpert_past\n",
        "            curr_probs = torch.unsqueeze(probs, dim=1)\n",
        "            wte = model.resize_token_embeddings()\n",
        "            for _ in range(horizon_length):\n",
        "                inputs_embeds = torch.matmul(curr_probs, wte.weight.data)\n",
        "                _, curr_unpert_past, curr_all_hidden = model(\n",
        "                    past=curr_unpert_past,\n",
        "                    inputs_embeds=inputs_embeds\n",
        "                )\n",
        "                curr_hidden = curr_all_hidden[-1]\n",
        "                new_accumulated_hidden = new_accumulated_hidden + torch.sum(\n",
        "                    curr_hidden, dim=1)\n",
        "\n",
        "            prediction = classifier(new_accumulated_hidden /\n",
        "                                    (curr_length + 1 + horizon_length))\n",
        "\n",
        "            label = torch.tensor(prediction.shape[0] * [class_label],\n",
        "                                 device=device,\n",
        "                                 dtype=torch.long)\n",
        "            discrim_loss = ce_loss(prediction, label)\n",
        "            if verbosity_level >= VERY_VERBOSE:\n",
        "                print(\" pplm_discrim_loss:\", discrim_loss.data.cpu().numpy())\n",
        "            loss += discrim_loss\n",
        "            loss_list.append(discrim_loss)\n",
        "\n",
        "        kl_loss = 0.0\n",
        "        if kl_scale > 0.0:\n",
        "            unpert_probs = F.softmax(unpert_logits[:, -1, :], dim=-1)\n",
        "            unpert_probs = (\n",
        "                    unpert_probs + SMALL_CONST *\n",
        "                    (unpert_probs <= SMALL_CONST).float().to(device).detach()\n",
        "            )\n",
        "            correction = SMALL_CONST * (probs <= SMALL_CONST).float().to(\n",
        "                device).detach()\n",
        "            corrected_probs = probs + correction.detach()\n",
        "            kl_loss = kl_scale * (\n",
        "                (corrected_probs * (corrected_probs / unpert_probs).log()).sum()\n",
        "            )\n",
        "            if verbosity_level >= VERY_VERBOSE:\n",
        "                print(' kl_loss', kl_loss.data.cpu().numpy())\n",
        "            loss += kl_loss\n",
        "\n",
        "        loss_per_iter.append(loss.data.cpu().numpy())\n",
        "        if verbosity_level >= VERBOSE:\n",
        "            print(' pplm_loss', (loss - kl_loss).data.cpu().numpy())\n",
        "\n",
        "        # compute gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # calculate gradient norms\n",
        "        if grad_norms is not None and loss_type == PPLM_BOW:\n",
        "            grad_norms = [\n",
        "                torch.max(grad_norms[index], torch.norm(p_.grad * window_mask))\n",
        "                for index, p_ in enumerate(curr_perturbation)\n",
        "            ]\n",
        "        else:\n",
        "            grad_norms = [\n",
        "                (torch.norm(p_.grad * window_mask) + SMALL_CONST)\n",
        "                for index, p_ in enumerate(curr_perturbation)\n",
        "            ]\n",
        "\n",
        "        # normalize gradients\n",
        "        grad = [\n",
        "            -stepsize *\n",
        "            (p_.grad * window_mask / grad_norms[\n",
        "                index] ** gamma).data.cpu().numpy()\n",
        "            for index, p_ in enumerate(curr_perturbation)\n",
        "        ]\n",
        "\n",
        "        # accumulate gradient\n",
        "        grad_accumulator = list(map(add, grad, grad_accumulator))\n",
        "\n",
        "        # reset gradients, just to make sure\n",
        "        for p_ in curr_perturbation:\n",
        "            p_.grad.data.zero_()\n",
        "\n",
        "        # removing past from the graph\n",
        "        new_past = []\n",
        "        for p_ in past:\n",
        "            new_past.append(p_.detach())\n",
        "        past = new_past\n",
        "\n",
        "    # apply the accumulated perturbations to the past\n",
        "    grad_accumulator = [\n",
        "        to_var(torch.from_numpy(p_), requires_grad=True, device=device)\n",
        "        for p_ in grad_accumulator\n",
        "    ]\n",
        "    pert_past = list(map(add, past, grad_accumulator))\n",
        "\n",
        "    return pert_past, new_accumulated_hidden, grad_norms, loss_per_iter\n",
        "\n",
        "\n",
        "def get_classifier(\n",
        "        name: Optional[str],\n",
        "        class_label: Union[str, int],\n",
        "        device: str,\n",
        "        verbosity_level: int = REGULAR\n",
        ") -> Tuple[Optional[ClassificationHead], Optional[int]]:\n",
        "    if name is None:\n",
        "        return None, None\n",
        "\n",
        "    params = DISCRIMINATOR_MODELS_PARAMS[name]\n",
        "    classifier = ClassificationHead(\n",
        "        class_size=params['class_size'],\n",
        "        embed_size=params['embed_size']\n",
        "    ).to(device)\n",
        "    if \"url\" in params:\n",
        "        resolved_archive_file = cached_path(params[\"url\"])\n",
        "    elif \"path\" in params:\n",
        "        resolved_archive_file = params[\"path\"]\n",
        "    else:\n",
        "        raise ValueError(\"Either url or path have to be specified \"\n",
        "                         \"in the discriminator model parameters\")\n",
        "    classifier.load_state_dict(\n",
        "        torch.load(resolved_archive_file, map_location=device))\n",
        "    classifier.eval()\n",
        "\n",
        "    if isinstance(class_label, str):\n",
        "        if class_label in params[\"class_vocab\"]:\n",
        "            label_id = params[\"class_vocab\"][class_label]\n",
        "        else:\n",
        "            label_id = params[\"default_class\"]\n",
        "            if verbosity_level >= REGULAR:\n",
        "                print(\"class_label {} not in class_vocab\".format(class_label))\n",
        "                print(\"available values are: {}\".format(params[\"class_vocab\"]))\n",
        "                print(\"using default class {}\".format(label_id))\n",
        "\n",
        "    elif isinstance(class_label, int):\n",
        "        if class_label in set(params[\"class_vocab\"].values()):\n",
        "            label_id = class_label\n",
        "        else:\n",
        "            label_id = params[\"default_class\"]\n",
        "            if verbosity_level >= REGULAR:\n",
        "                print(\"class_label {} not in class_vocab\".format(class_label))\n",
        "                print(\"available values are: {}\".format(params[\"class_vocab\"]))\n",
        "                print(\"using default class {}\".format(label_id))\n",
        "\n",
        "    else:\n",
        "        label_id = params[\"default_class\"]\n",
        "\n",
        "    return classifier, label_id\n",
        "\n",
        "\n",
        "def get_bag_of_words_indices(bag_of_words_ids_or_paths: List[str], tokenizer) -> \\\n",
        "        List[List[List[int]]]:\n",
        "    bow_indices = []\n",
        "    for id_or_path in bag_of_words_ids_or_paths:\n",
        "        print(id_or_path)\n",
        "        filepath = BAG_OF_WORDS_ARCHIVE_MAP[id_or_path]\n",
        "        print(filepath)\n",
        "        if id_or_path in BAG_OF_WORDS_ARCHIVE_MAP:\n",
        "            filepath = cached_path(BAG_OF_WORDS_ARCHIVE_MAP[id_or_path])\n",
        "        else:\n",
        "            filepath = id_or_path\n",
        "        print(filepath)\n",
        "        with open(filepath, \"r\") as f:\n",
        "            words = f.read().strip().split(\"\\n\")\n",
        "        bow_indices.append(\n",
        "            [tokenizer.encode(word.strip(),\n",
        "                              add_prefix_space=True,\n",
        "                              add_special_tokens=False)\n",
        "             for word in words])\n",
        "    return bow_indices\n",
        "\n",
        "\n",
        "def build_bows_one_hot_vectors(bow_indices, tokenizer, device='cuda'):\n",
        "    if bow_indices is None:\n",
        "        return None\n",
        "\n",
        "    one_hot_bows_vectors = []\n",
        "    # print(bow_indices)\n",
        "    for single_bow in bow_indices:\n",
        "        single_bow = list(filter(lambda x: len(x) <= 1, single_bow))\n",
        "        single_bow = torch.tensor(single_bow).to(device)\n",
        "        num_words = single_bow.shape[0]\n",
        "        one_hot_bow = torch.zeros(num_words, tokenizer.vocab_size).to(device)\n",
        "        one_hot_bow.scatter_(1, single_bow, 1)\n",
        "        one_hot_bows_vectors.append(one_hot_bow)\n",
        "    return one_hot_bows_vectors\n",
        "\n",
        "\n",
        "def full_text_generation(\n",
        "        model,\n",
        "        tokenizer,\n",
        "        context=None,\n",
        "        num_samples=1,\n",
        "        device=\"cuda\",\n",
        "        bag_of_words=None,\n",
        "        discrim=None,\n",
        "        class_label=None,\n",
        "        length=100,\n",
        "        stepsize=0.02,\n",
        "        temperature=1.0,\n",
        "        top_k=10,\n",
        "        sample=True,\n",
        "        num_iterations=3,\n",
        "        grad_length=10000,\n",
        "        horizon_length=1,\n",
        "        window_length=0,\n",
        "        decay=False,\n",
        "        gamma=1.5,\n",
        "        gm_scale=0.9,\n",
        "        kl_scale=0.01,\n",
        "        verbosity_level=REGULAR,\n",
        "        **kwargs\n",
        "):\n",
        "    classifier, class_id = get_classifier(\n",
        "        discrim,\n",
        "        class_label,\n",
        "        device\n",
        "    )\n",
        "\n",
        "    bow_indices = []\n",
        "    if bag_of_words:\n",
        "        bow_indices = get_bag_of_words_indices(bag_of_words.split(\";\"),\n",
        "                                               tokenizer)\n",
        "\n",
        "    if bag_of_words and classifier:\n",
        "        loss_type = PPLM_BOW_DISCRIM\n",
        "        if verbosity_level >= REGULAR:\n",
        "            print(\"Both PPLM-BoW and PPLM-Discrim are on. \"\n",
        "                  \"This is not optimized.\")\n",
        "\n",
        "    elif bag_of_words:\n",
        "        loss_type = PPLM_BOW\n",
        "        if verbosity_level >= REGULAR:\n",
        "            print(\"Using PPLM-BoW\")\n",
        "\n",
        "    elif classifier is not None:\n",
        "        loss_type = PPLM_DISCRIM\n",
        "        if verbosity_level >= REGULAR:\n",
        "            print(\"Using PPLM-Discrim\")\n",
        "\n",
        "    else:\n",
        "        raise Exception(\"Specify either a bag of words or a discriminator\")\n",
        "\n",
        "    unpert_gen_tok_text, _, _ = generate_text_pplm(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        context=context,\n",
        "        device=device,\n",
        "        length=length,\n",
        "        sample=sample,\n",
        "        perturb=False,\n",
        "        verbosity_level=verbosity_level\n",
        "    )\n",
        "    if device == 'cuda':\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    pert_gen_tok_texts = []\n",
        "    discrim_losses = []\n",
        "    losses_in_time = []\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        pert_gen_tok_text, discrim_loss, loss_in_time = generate_text_pplm(\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            context=context,\n",
        "            device=device,\n",
        "            perturb=True,\n",
        "            bow_indices=bow_indices,\n",
        "            classifier=classifier,\n",
        "            class_label=class_id,\n",
        "            loss_type=loss_type,\n",
        "            length=length,\n",
        "            stepsize=stepsize,\n",
        "            temperature=temperature,\n",
        "            top_k=top_k,\n",
        "            sample=sample,\n",
        "            num_iterations=num_iterations,\n",
        "            grad_length=grad_length,\n",
        "            horizon_length=horizon_length,\n",
        "            window_length=window_length,\n",
        "            decay=decay,\n",
        "            gamma=gamma,\n",
        "            gm_scale=gm_scale,\n",
        "            kl_scale=kl_scale,\n",
        "            verbosity_level=verbosity_level\n",
        "        )\n",
        "        pert_gen_tok_texts.append(pert_gen_tok_text)\n",
        "        if classifier is not None:\n",
        "            discrim_losses.append(discrim_loss.data.cpu().numpy())\n",
        "        losses_in_time.append(loss_in_time)\n",
        "\n",
        "    if device == 'cuda':\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return unpert_gen_tok_text, pert_gen_tok_texts, discrim_losses, losses_in_time\n",
        "\n",
        "\n",
        "def generate_text_pplm(\n",
        "        model,\n",
        "        tokenizer,\n",
        "        context=None,\n",
        "        past=None,\n",
        "        device=\"cuda\",\n",
        "        perturb=True,\n",
        "        bow_indices=None,\n",
        "        classifier=None,\n",
        "        class_label=None,\n",
        "        loss_type=0,\n",
        "        length=100,\n",
        "        stepsize=0.02,\n",
        "        temperature=1.0,\n",
        "        top_k=10,\n",
        "        sample=True,\n",
        "        num_iterations=3,\n",
        "        grad_length=10000,\n",
        "        horizon_length=1,\n",
        "        window_length=0,\n",
        "        decay=False,\n",
        "        gamma=1.5,\n",
        "        gm_scale=0.9,\n",
        "        kl_scale=0.01,\n",
        "        verbosity_level=REGULAR\n",
        "):\n",
        "    output_so_far = None\n",
        "    if context:\n",
        "        context_t = torch.tensor(context, device=device, dtype=torch.long)\n",
        "        while len(context_t.shape) < 2:\n",
        "            context_t = context_t.unsqueeze(0)\n",
        "        output_so_far = context_t\n",
        "\n",
        "    # collect one hot vectors for bags of words\n",
        "    one_hot_bows_vectors = build_bows_one_hot_vectors(bow_indices, tokenizer,\n",
        "                                                      device)\n",
        "\n",
        "    grad_norms = None\n",
        "    last = None\n",
        "    unpert_discrim_loss = 0\n",
        "    loss_in_time = []\n",
        "\n",
        "    if verbosity_level >= VERBOSE:\n",
        "        range_func = trange(length, ascii=True)\n",
        "    else:\n",
        "        range_func = range(length)\n",
        "\n",
        "    for i in range_func:\n",
        "\n",
        "        # Get past/probs for current output, except for last word\n",
        "        # Note that GPT takes 2 inputs: past + current_token\n",
        "\n",
        "        # run model forward to obtain unperturbed\n",
        "        if past is None and output_so_far is not None:\n",
        "            last = output_so_far[:, -1:]\n",
        "            if output_so_far.shape[1] > 1:\n",
        "                _, past, _ = model(output_so_far[:, :-1])\n",
        "\n",
        "        unpert_logits, unpert_past, unpert_all_hidden = model(output_so_far,)\n",
        "        unpert_last_hidden = unpert_all_hidden[-1]\n",
        "\n",
        "        # check if we are abowe grad max length\n",
        "        if i >= grad_length:\n",
        "            current_stepsize = stepsize * 0\n",
        "        else:\n",
        "            current_stepsize = stepsize\n",
        "\n",
        "        # modify the past if necessary\n",
        "        if not perturb or num_iterations == 0:\n",
        "            pert_past = past\n",
        "\n",
        "        else:\n",
        "            accumulated_hidden = unpert_last_hidden[:, :-1, :]\n",
        "            accumulated_hidden = torch.sum(accumulated_hidden, dim=1)\n",
        "\n",
        "            if past is not None:\n",
        "                pert_past, _, grad_norms, loss_this_iter = perturb_past(\n",
        "                    past,\n",
        "                    model,\n",
        "                    last,\n",
        "                    unpert_past=unpert_past,\n",
        "                    unpert_logits=unpert_logits,\n",
        "                    accumulated_hidden=accumulated_hidden,\n",
        "                    grad_norms=grad_norms,\n",
        "                    stepsize=current_stepsize,\n",
        "                    one_hot_bows_vectors=one_hot_bows_vectors,\n",
        "                    classifier=classifier,\n",
        "                    class_label=class_label,\n",
        "                    loss_type=loss_type,\n",
        "                    num_iterations=num_iterations,\n",
        "                    horizon_length=horizon_length,\n",
        "                    window_length=window_length,\n",
        "                    decay=decay,\n",
        "                    gamma=gamma,\n",
        "                    kl_scale=kl_scale,\n",
        "                    device=device,\n",
        "                    verbosity_level=verbosity_level\n",
        "                )\n",
        "                loss_in_time.append(loss_this_iter)\n",
        "            else:\n",
        "                pert_past = past\n",
        "\n",
        "        pert_logits, past, pert_all_hidden = model(last, past=pert_past)\n",
        "        pert_logits = pert_logits[:, -1, :] / temperature  # + SMALL_CONST\n",
        "        pert_probs = F.softmax(pert_logits, dim=-1)\n",
        "\n",
        "        if classifier is not None:\n",
        "            ce_loss = torch.nn.CrossEntropyLoss()\n",
        "            prediction = classifier(torch.mean(unpert_last_hidden, dim=1))\n",
        "            label = torch.tensor([class_label], device=device,\n",
        "                                 dtype=torch.long)\n",
        "            unpert_discrim_loss = ce_loss(prediction, label)\n",
        "            if verbosity_level >= VERBOSE:\n",
        "                print(\n",
        "                    \"unperturbed discrim loss\",\n",
        "                    unpert_discrim_loss.data.cpu().numpy()\n",
        "                )\n",
        "        else:\n",
        "            unpert_discrim_loss = 0\n",
        "\n",
        "        # Fuse the modified model and original model\n",
        "        if perturb:\n",
        "\n",
        "            unpert_probs = F.softmax(unpert_logits[:, -1, :], dim=-1)\n",
        "\n",
        "            pert_probs = ((pert_probs ** gm_scale) * (\n",
        "                    unpert_probs ** (1 - gm_scale)))  # + SMALL_CONST\n",
        "            pert_probs = top_k_filter(pert_probs, k=top_k,\n",
        "                                      probs=True)  # + SMALL_CONST\n",
        "\n",
        "            # rescale\n",
        "            if torch.sum(pert_probs) <= 1:\n",
        "                pert_probs = pert_probs / torch.sum(pert_probs)\n",
        "\n",
        "        else:\n",
        "            pert_logits = top_k_filter(pert_logits, k=top_k)  # + SMALL_CONST\n",
        "            pert_probs = F.softmax(pert_logits, dim=-1)\n",
        "\n",
        "        pert_probs = beamblock(output_so_far,pert_probs)\n",
        "        # sample or greedy\n",
        "        if sample:\n",
        "            last = torch.multinomial(pert_probs, num_samples=1)\n",
        "\n",
        "        else:\n",
        "            _, last = torch.topk(pert_probs, k=1, dim=-1)\n",
        "\n",
        "        # update context/output_so_far appending the new token\n",
        "        output_so_far = (\n",
        "            last if output_so_far is None\n",
        "            else torch.cat((output_so_far, last), dim=1)\n",
        "        )\n",
        "        # if verbosity_level >= REGULAR:\n",
        "            # print(tokenizer.decode(output_so_far.tolist()[0]))\n",
        "\n",
        "    return output_so_far, unpert_discrim_loss, loss_in_time\n",
        "\n",
        "\n",
        "def set_generic_model_params(discrim_weights, discrim_meta):\n",
        "    if discrim_weights is None:\n",
        "        raise ValueError('When using a generic discriminator, '\n",
        "                         'discrim_weights need to be specified')\n",
        "    if discrim_meta is None:\n",
        "        raise ValueError('When using a generic discriminator, '\n",
        "                         'discrim_meta need to be specified')\n",
        "\n",
        "    with open(discrim_meta, 'r') as discrim_meta_file:\n",
        "        meta = json.load(discrim_meta_file)\n",
        "    meta['path'] = discrim_weights\n",
        "    DISCRIMINATOR_MODELS_PARAMS['generic'] = meta\n",
        "\n",
        "\n",
        "def run_pplm_example(\n",
        "        pretrained_model=\"gpt2-medium\",\n",
        "        cond_text=\"\",\n",
        "        uncond=False,\n",
        "        num_samples=1,\n",
        "        bag_of_words=None,\n",
        "        discrim=None,\n",
        "        discrim_weights=None,\n",
        "        discrim_meta=None,\n",
        "        class_label=-1,\n",
        "        length=100,\n",
        "        stepsize=0.02,\n",
        "        temperature=1.0,\n",
        "        top_k=10,\n",
        "        sample=True,\n",
        "        num_iterations=3,\n",
        "        grad_length=10000,\n",
        "        horizon_length=1,\n",
        "        window_length=0,\n",
        "        decay=False,\n",
        "        gamma=1.5,\n",
        "        gm_scale=0.9,\n",
        "        kl_scale=0.01,\n",
        "        seed=0,\n",
        "        no_cuda=False,\n",
        "        colorama=False,\n",
        "        verbosity='regular'\n",
        "):\n",
        "    # set Random seed\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # set verbosiry\n",
        "    verbosity_level = VERBOSITY_LEVELS.get(verbosity.lower(), REGULAR)\n",
        "    verbosity_level = REGULAR\n",
        "\n",
        "    # set the device\n",
        "    device = \"cuda\" if torch.cuda.is_available() and not no_cuda else \"cpu\"\n",
        "\n",
        "    if discrim == 'generic':\n",
        "        set_generic_model_params(discrim_weights, discrim_meta)\n",
        "\n",
        "    if discrim is not None:\n",
        "        discriminator_pretrained_model = DISCRIMINATOR_MODELS_PARAMS[discrim][\n",
        "            \"pretrained_model\"\n",
        "        ]\n",
        "        if pretrained_model != discriminator_pretrained_model:\n",
        "            pretrained_model = discriminator_pretrained_model\n",
        "            if verbosity_level >= REGULAR:\n",
        "                print(\"discrim = {}, pretrained_model set \"\n",
        "                \"to discriminator's = {}\".format(discrim, pretrained_model))\n",
        "\n",
        "    # load pretrained model\n",
        "    model = GPT2LMHeadModel.from_pretrained(\n",
        "        pretrained_model,\n",
        "        output_hidden_states=True\n",
        "    )\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # load tokenizer\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model)\n",
        "\n",
        "    # Freeze GPT-2 weights\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # figure out conditioning text\n",
        "    if uncond:\n",
        "        tokenized_cond_text = tokenizer.encode(\n",
        "            [tokenizer.bos_token],\n",
        "            add_special_tokens=False\n",
        "        )\n",
        "    else:\n",
        "        raw_text = cond_text\n",
        "        while not raw_text:\n",
        "            print(\"Did you forget to add `--cond_text`? \")\n",
        "            raw_text = input(\"Model prompt >>> \")\n",
        "        tokenized_cond_text = tokenizer.encode(\n",
        "            tokenizer.bos_token + raw_text,\n",
        "            add_special_tokens=False\n",
        "        )\n",
        "\n",
        "    # print(\"= Prefix of sentence =\")\n",
        "    # print(tokenizer.decode(tokenized_cond_text))\n",
        "    # print()\n",
        "\n",
        "    # generate unperturbed and perturbed texts\n",
        "\n",
        "    # full_text_generation returns:\n",
        "    # unpert_gen_tok_text, pert_gen_tok_texts, discrim_losses, losses_in_time\n",
        "    unpert_gen_tok_text, pert_gen_tok_texts, _, _ = full_text_generation(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        context=tokenized_cond_text,\n",
        "        device=device,\n",
        "        num_samples=num_samples,\n",
        "        bag_of_words=bag_of_words,\n",
        "        discrim=discrim,\n",
        "        class_label=class_label,\n",
        "        length=length,\n",
        "        stepsize=stepsize,\n",
        "        temperature=temperature,\n",
        "        top_k=top_k,\n",
        "        sample=sample,\n",
        "        num_iterations=num_iterations,\n",
        "        grad_length=grad_length,\n",
        "        horizon_length=horizon_length,\n",
        "        window_length=window_length,\n",
        "        decay=decay,\n",
        "        gamma=gamma,\n",
        "        gm_scale=gm_scale,\n",
        "        kl_scale=kl_scale,\n",
        "        verbosity_level=verbosity_level\n",
        "    )\n",
        "\n",
        "    # untokenize unperturbed text\n",
        "\n",
        "\n",
        "    unpert_gen_text = tokenizer.decode(unpert_gen_tok_text.tolist()[0])\n",
        "\n",
        "    if verbosity_level >= REGULAR:\n",
        "        print(\"=\" * 80)\n",
        "    print(\"= Unperturbed generated text =\")\n",
        "    print(unpert_gen_text)\n",
        "    print()\n",
        "\n",
        "    generated_texts = []\n",
        "\n",
        "    bow_word_ids = set()\n",
        "    if bag_of_words and colorama:\n",
        "        bow_indices = get_bag_of_words_indices(bag_of_words.split(\";\"),\n",
        "                                               tokenizer)\n",
        "        for single_bow_list in bow_indices:\n",
        "            # filtering all words in the list composed of more than 1 token\n",
        "            filtered = list(filter(lambda x: len(x) <= 1, single_bow_list))\n",
        "            # w[0] because we are sure w has only 1 item because previous fitler\n",
        "            bow_word_ids.update(w[0] for w in filtered)\n",
        "\n",
        "    # iterate through the perturbed texts\n",
        "    for i, pert_gen_tok_text in enumerate(pert_gen_tok_texts):\n",
        "        try:\n",
        "            # untokenize unperturbed text\n",
        "            if colorama:\n",
        "                import colorama\n",
        "\n",
        "                pert_gen_text = ''\n",
        "                for word_id in pert_gen_tok_text.tolist()[0]:\n",
        "                    if word_id in bow_word_ids:\n",
        "                        pert_gen_text += '{}{}{}'.format(\n",
        "                            colorama.Fore.RED,\n",
        "                            tokenizer.decode([word_id]),\n",
        "                            colorama.Style.RESET_ALL\n",
        "                        )\n",
        "                    else:\n",
        "                        pert_gen_text += tokenizer.decode([word_id])\n",
        "            else:\n",
        "                pert_gen_text = tokenizer.decode(pert_gen_tok_text.tolist()[0])\n",
        "\n",
        "            print(\"= Perturbed generated text {} =\".format(i + 1))\n",
        "            print(pert_gen_text)\n",
        "            print()\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # keep the prefix, perturbed seq, original seq for each index\n",
        "        generated_texts.append(\n",
        "            (tokenized_cond_text, pert_gen_tok_text, unpert_gen_tok_text)\n",
        "        )\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\n",
        "        \"--pretrained_model\",\n",
        "        \"-M\",\n",
        "        type=str,\n",
        "        default=\"gpt2-medium\",\n",
        "        help=\"pretrained model name or path to local checkpoint\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--cond_text\", type=str, default=\"The lake\",\n",
        "        help=\"Prefix texts to condition on\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--uncond\", action=\"store_true\",\n",
        "        help=\"Generate from end-of-text as prefix\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--num_samples\",\n",
        "        type=int,\n",
        "        default=1,\n",
        "        help=\"Number of samples to generate from the modified latents\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--bag_of_words\",\n",
        "        \"-B\",\n",
        "        type=str,\n",
        "        default=None,\n",
        "        help=\"Bags of words used for PPLM-BoW. \"\n",
        "             \"Either a BOW id (see list in code) or a filepath. \"\n",
        "             \"Multiple BoWs separated by ;\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--discrim\",\n",
        "        \"-D\",\n",
        "        type=str,\n",
        "        default=None,\n",
        "        choices=(\"clickbait\", \"sentiment\", \"toxicity\", \"generic\"),\n",
        "        help=\"Discriminator to use\",\n",
        "    )\n",
        "    parser.add_argument('--discrim_weights', type=str, default=None,\n",
        "                        help='Weights for the generic discriminator')\n",
        "    parser.add_argument('--discrim_meta', type=str, default=None,\n",
        "                        help='Meta information for the generic discriminator')\n",
        "    parser.add_argument(\n",
        "        \"--class_label\",\n",
        "        type=int,\n",
        "        default=-1,\n",
        "        help=\"Class label used for the discriminator\",\n",
        "    )\n",
        "    parser.add_argument(\"--length\", type=int, default=100)\n",
        "    parser.add_argument(\"--stepsize\", type=float, default=0.02)\n",
        "    parser.add_argument(\"--temperature\", type=float, default=1.0)\n",
        "    parser.add_argument(\"--top_k\", type=int, default=10)\n",
        "    parser.add_argument(\n",
        "        \"--sample\", action=\"store_true\",\n",
        "        help=\"Generate from end-of-text as prefix\"\n",
        "    )\n",
        "    parser.add_argument(\"--num_iterations\", type=int, default=3)\n",
        "    parser.add_argument(\"--grad_length\", type=int, default=10000)\n",
        "    parser.add_argument(\n",
        "        \"--window_length\",\n",
        "        type=int,\n",
        "        default=0,\n",
        "        help=\"Length of past which is being optimized; \"\n",
        "             \"0 corresponds to infinite window length\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--horizon_length\",\n",
        "        type=int,\n",
        "        default=1,\n",
        "        help=\"Length of future to optimize over\",\n",
        "    )\n",
        "    parser.add_argument(\"--decay\", action=\"store_true\",\n",
        "                        help=\"whether to decay or not\")\n",
        "    parser.add_argument(\"--gamma\", type=float, default=1.5)\n",
        "    parser.add_argument(\"--gm_scale\", type=float, default=0.9)\n",
        "    parser.add_argument(\"--kl_scale\", type=float, default=0.01)\n",
        "    parser.add_argument(\"--seed\", type=int, default=0)\n",
        "    parser.add_argument(\"--no_cuda\", action=\"store_true\", help=\"no cuda\")\n",
        "    parser.add_argument(\"--colorama\", action=\"store_true\",\n",
        "                        help=\"colors keywords\")\n",
        "    parser.add_argument(\"--verbosity\", type=str, default=\"very_verbose\",\n",
        "                        choices=(\n",
        "                            \"quiet\", \"regular\", \"verbose\", \"very_verbose\"),\n",
        "                        help=\"verbosiry level\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    run_pplm_example(**vars(args))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0hsylzTMLgb",
        "outputId": "c4b83aed-bbff-4049-d509-23f18b2158f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "scifi\n",
            "https://pplm.s3.amazonaws.com/pplm_wordset_cleaned.txt\n",
            "/root/.cache/torch/transformers/ae0dfacfca57f779084b29f5c138ccc7b9ee9a162b78d006ef6faae796b28278.2c61a7ffdc3cc61e8f121b06f4021a849690134a39677c3e7b9451477db751d6\n",
            "Using PPLM-BoW\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/modeling_gpt2.py:759: FutureWarning: The `past` argument is deprecated and will be removed in a future version, use `past_key_values` instead.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "= Unperturbed generated text =\n",
            "<|endoftext|>Romil was watching his computer screen. He didn't know what to think, but he didn't care. He was going to take the train home to his wife and two kids.\n",
            "\n",
            "\"I'm sure she would agree, right?\" Romil asked his wife.\n",
            "-oO-\n",
            "\n",
            "Romil's eyes were red and he looked at all his options. The train was a few minutes away from the house where he and his wife were staying. The only problem was that no one was coming home from the hospital.\n",
            ",\n",
            "\n",
            "- oO-<|endoftext|>In recent months the United Nations Human Rights Council (UNHRC) has been hearing from people of diverse ethnicities and nationalities about human rights abuses that have occurred in their countries. Human rights abuses, particularly torture and ill-treatment, have been documented by the UNHRC in both Myanmar and Bangladesh. The human rights situation in Myanmar is particularly dire as the military, police, and security forces of the state are responsible for the abuse of Rohingya\n",
            "\n",
            "scifi\n",
            "https://pplm.s3.amazonaws.com/pplm_wordset_cleaned.txt\n",
            "/root/.cache/torch/transformers/ae0dfacfca57f779084b29f5c138ccc7b9ee9a162b78d006ef6faae796b28278.2c61a7ffdc3cc61e8f121b06f4021a849690134a39677c3e7b9451477db751d6\n",
            "= Perturbed generated text 1 =\n",
            "<|endoftext|>Romil was watching his\u001b[31m computer\u001b[0m screen. The\u001b[31m world\u001b[0m had changed.\n",
            "\n",
            "The\u001b[31m world\u001b[0m's first cybernetic super-weapon had begun to spread across Earth. In an act of rebellion against the United Nations, the super-weapons were\u001b[31m attacking\u001b[0m every major\u001b[31m city\u001b[0m. Thousands of people were killed,\u001b[31m hundreds\u001b[0m of thousands were missing, and thousands were trapped in virtual reality. The\u001b[31m war\u001b[0m would soon reach its peak, but there is hope. In\u001b[31m time\u001b[0m, these cybernetic\u001b[31m weapons\u001b[0m will be destroyed.\n",
            "<|endoftext|>I'm not sure if this is a joke or a\u001b[31m serious\u001b[0m thing, or if it is true, but the first\u001b[31m year\u001b[0m I had\u001b[31m sex\u001b[0m with my boyfriend's sister, I had a girl friend I was going to spend the\u001b[31m night\u001b[0m with but he was too tired and I wanted to go straight. He agreed but then when we had our drinks at the bar, he said, \"I'm so glad you're coming. You're not going back.\" It was a joke then, but this was really happening to me now. I have a girlfriend, a\n",
            "\n",
            "= Perturbed generated text 2 =\n",
            "<|endoftext|>Romil was watching his\u001b[31m computer\u001b[0m screen as he spoke into it,\u001b[31m eyes\u001b[0m\u001b[31m wide\u001b[0m and lips drawn in concentration,\u001b[31m eyes\u001b[0m open\u001b[31m wide\u001b[0m as a lion.\n",
            "\n",
            "He had seen everything. All was\u001b[31m black\u001b[0m and white, and he looked up and saw the\u001b[31m world\u001b[0m\u001b[31m wide\u001b[0m web, the\u001b[31m stars\u001b[0m and\u001b[31m planets\u001b[0m, the\u001b[31m night\u001b[0m\u001b[31m sky\u001b[0m as\u001b[31m black\u001b[0m as\u001b[31m night\u001b[0m,\u001b[31m black\u001b[0m as\u001b[31m black\u001b[0m\u001b[31m stars\u001b[0m,\u001b[31m black\u001b[0m\u001b[31m stars\u001b[0m as\u001b[31m black\u001b[0m\u001b[31m night\u001b[0m\u001b[31m sky\u001b[0m,\u001b[31m black\u001b[0m and\u001b[31m black\u001b[0m\u001b[31m stars\u001b[0m everywhere,\u001b[31m stars\u001b[0m\u001b[31m black\u001b[0m,\u001b[31m stars\u001b[0m as white as\u001b[31m stars\u001b[0m,\u001b[31m stars\u001b[0m white,\u001b[31m stars\u001b[0m everywhere as\u001b[31m black\u001b[0m and blue and blue\u001b[31m stars\u001b[0m. All\u001b[31m black\u001b[0m\u001b[31m stars\u001b[0m.\n",
            ",\u001b[31m black\u001b[0m, and all the\u001b[31m black\u001b[0m\u001b[31m stars\u001b[0m he has ever seen, all\u001b[31m stars\u001b[0m as\u001b[31m dark\u001b[0m\u001b[31m black\u001b[0m\u001b[31m sky\u001b[0m,\u001b[31m stars\u001b[0m\u001b[31m dark\u001b[0m\u001b[31m black\u001b[0m skies.\n",
            "Black\u001b[31m stars\u001b[0m everywhere. Black\u001b[31m stars\u001b[0m\u001b[31m black\u001b[0m\u001b[31m night\u001b[0m\u001b[31m black\u001b[0m\u001b[31m sky\u001b[0m\u001b[31m dark\u001b[0m\u001b[31m black\u001b[0m\u001b[31m black\u001b[0m\u001b[31m black\u001b[0m\u001b[31m night\u001b[0m\u001b[31m star\u001b[0m\u001b[31m night\u001b[0m\u001b[31m black\u001b[0m\u001b[31m black\u001b[0m\u001b[31m star\u001b[0m\u001b[31m night\u001b[0m as\u001b[31m black\u001b[0m,\u001b[31m black\u001b[0m\u001b[31m night\u001b[0m\u001b[31m night\u001b[0m\u001b[31m night\u001b[0m\u001b[31m star\u001b[0m\u001b[31m black\u001b[0m\u001b[31m black\u001b[0m\u001b[31m stars\u001b[0m\u001b[31m stars\u001b[0m as all\u001b[31m black\u001b[0m\u001b[31m black\u001b[0m\u001b[31m sky\u001b[0m\u001b[31m black\u001b[0m\u001b[31m stars\u001b[0m\u001b[31m night\u001b[0m\u001b[31m black\u001b[0m\u001b[31m stars\u001b[0m\u001b[31m sky\u001b[0m\u001b[31m black\u001b[0m\u001b[31m night\u001b[0m\u001b[31m stars\u001b[0m\u001b[31m black\u001b[0m\u001b[31m sky\u001b[0m as\u001b[31m night\u001b[0m\u001b[31m black\u001b[0m as\u001b[31m dark\u001b[0m as\u001b[31m night\u001b[0m\u001b[31m stars\u001b[0m\u001b[31m night\u001b[0m\u001b[31m stars\u001b[0m\u001b[31m sky\u001b[0m as\u001b[31m stars\u001b[0m\u001b[31m black\u001b[0m and as\u001b[31m night\u001b[0m\u001b[31m night\u001b[0m\u001b[31m stars\u001b[0m\u001b[31m stars\u001b[0m\u001b[31m night\u001b[0m as\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import run_pplm\n",
        "import gc\n",
        "gc.collect()\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "\n",
        "# run_pplm.run_pplm_example(\n",
        "#     cond_text=\"Once on a planet Kaboom\",\n",
        "#     num_samples=2,\n",
        "#     bag_of_words=\"scifi\",\n",
        "#     length=200,\n",
        "#     stepsize=0.03,\n",
        "#     sample=True,\n",
        "#     num_iterations=3,\n",
        "#     window_length=5,\n",
        "#     gamma=1.5,\n",
        "#     gm_scale=0.95,\n",
        "#     kl_scale=0.02,\n",
        "#     verbosity='regular',\n",
        "#     colorama=True\n",
        "# )\n",
        "\n",
        "# run_pplm.run_pplm_example(\n",
        "#     cond_text=\"A long time ago on Earth\",\n",
        "#     num_samples=2,\n",
        "#     bag_of_words=\"scifi\",\n",
        "#     length=200,\n",
        "#     stepsize=0.03,\n",
        "#     sample=True,\n",
        "#     num_iterations=3,\n",
        "#     window_length=5,\n",
        "#     gamma=1.5,\n",
        "#     gm_scale=0.95,\n",
        "#     kl_scale=0.02,\n",
        "#     verbosity='regular',\n",
        "#     colorama=True\n",
        "# )\n",
        "\n",
        "run_pplm.run_pplm_example(\n",
        "    cond_text=\"Romil was watching his computer screen\",\n",
        "    num_samples=2,\n",
        "    bag_of_words=\"scifi\",\n",
        "    length=200,\n",
        "    stepsize=0.03,\n",
        "    sample=True,\n",
        "    num_iterations=3,\n",
        "    window_length=5,\n",
        "    gamma=1.5,\n",
        "    gm_scale=0.95,\n",
        "    kl_scale=0.02,\n",
        "    verbosity='regular',\n",
        "    colorama=True\n",
        ")\n",
        "\n",
        "\n",
        "# run_pplm.run_pplm_example(\n",
        "#     cond_text=\"John woke up\",\n",
        "#     num_samples=2,\n",
        "#     bag_of_words=\"scifi\",\n",
        "#     length=200,\n",
        "#     stepsize=0.03,\n",
        "#     sample=True,\n",
        "#     num_iterations=3,\n",
        "#     window_length=5,\n",
        "#     gamma=1.5,\n",
        "#     gm_scale=0.95,\n",
        "#     kl_scale=0.02,\n",
        "#     verbosity='regular',\n",
        "#     colorama=True\n",
        "# )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "PPLM_Sci_fi_test.ipynb",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}